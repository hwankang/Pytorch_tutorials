{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hwan0816optimizer.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNF3jNIC/TEJZK8NW4pnMiZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwankang/Pytorch_tutorials/blob/main/hwan0816optimizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-dApEhJgyXh"
      },
      "outputs": [],
      "source": [
        "#https://hihack.tistory.com/entry/Pytorch-MNIST-데이터-셋-4가지-optimizer-성능-비교SGD-Adagrad-RMSprop-Adam [Goin0915:티스토리]\n",
        "# -*- coding: utf-8 -*-\n",
        "# ---\n",
        "# jupyter:\n",
        "#   jupytext:\n",
        "#     formats: ipynb,py:light\n",
        "#     text_representation:\n",
        "#       extension: .py\n",
        "#       format_name: light\n",
        "#       format_version: '1.5'\n",
        "#       jupytext_version: 1.11.2\n",
        "#   kernelspec:\n",
        "#     display_name: Python 3\n",
        "#     language: python\n",
        "#     name: python3\n",
        "# ---\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import os\n",
        "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784,100)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(100,100)\n",
        "        self.fc3 = nn.Linear(100,10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.fc1(x)\n",
        "        x2 = self.relu(x1)\n",
        "        x3 = self.fc2(x2)\n",
        "        x4 = self.relu(x3)\n",
        "        x5 = self.fc3(x4)\n",
        "\n",
        "        return x5\n",
        "\n",
        "\n",
        "download_root = './MNIST_data'\n",
        "\n",
        "dataset1 = datasets.MNIST(root = download_root,\n",
        "                         train=True,\n",
        "                         transform = transforms.ToTensor(),\n",
        "                         download=True)\n",
        "\n",
        "dataset2 = datasets.MNIST(root=download_root,\n",
        "                         train=False,\n",
        "                         transform = transforms.ToTensor(),\n",
        "                         download=True)\n",
        "\n",
        "batch_s = 100\n",
        "# dataset1_loader의 len은 600\n",
        "# dataset2_loader의 len은 100\n",
        "dataset1_loader = DataLoader(dataset1, batch_size = batch_s)\n",
        "dataset2_loader = DataLoader(dataset2, batch_size = batch_s)\n",
        "\n",
        "model_dict = {}\n",
        "loss_dict = {}\n",
        "accuracy_dict = {}\n",
        "# optimizer에 따른 학습 정도를 살펴볼 4가지 테스트 케이스\n",
        "optimizer_case = ['SGD','Adam','AdaGrad','RMSprop']\n",
        "for key in optimizer_case:\n",
        "    model_dict[key] = Net()\n",
        "    loss_dict[key] = []\n",
        "    accuracy_dict[key] = []\n",
        "\n",
        "# 4가지 테스트케이스에 대한 optimizer 정의\n",
        "optimizer_dict = {}\n",
        "optimizer_dict['SGD'] = optim.SGD(model_dict['SGD'].parameters(),lr = 0.005 )\n",
        "optimizer_dict['Adam'] = optim.Adam(model_dict['Adam'].parameters(),lr= 0.005)\n",
        "optimizer_dict['AdaGrad'] = optim.Adagrad(model_dict['AdaGrad'].parameters(), lr=0.005)\n",
        "optimizer_dict['RMSprop'] = optim.RMSprop(model_dict['RMSprop'].parameters(),lr=0.005)\n",
        "\n",
        "# loss_function, total batch size, epoch 정의\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "total_batch = len(dataset1_loader) # 600 (60000 / 100) => (train dataset / batch_size)\n",
        "epochs = np.arange(1,16)\n",
        "\n",
        "# 총 4가지 optimizer를 사용하여 학습\n",
        "for optimizer_name, optimizer in optimizer_dict.items():\n",
        "    print(optimizer_name)\n",
        "    for epoch in epochs:\n",
        "        cost=0\n",
        "        for images, labels in dataset1_loader: #dataloader는 image와 label로 구성\n",
        "            # 하나의 Tensor에 데이터 784(28x28)개가 담긴 리스트가 100개 들어있음\n",
        "            # 그리고 그것이 총 dataset1_loader의 len인 600개 존재\n",
        "            images = images.reshape(100,784) \n",
        "\n",
        "            model_dict[optimizer_name].zero_grad()\n",
        "\n",
        "            # feed forward\n",
        "            predict = model_dict[optimizer_name].forward(images)\n",
        "\n",
        "            # loss 값 구하기\n",
        "            loss = loss_function(predict,labels) # 예측된 것과 label이 얼마나 차이가 나는지\n",
        "\n",
        "            # back propagation\n",
        "            loss.backward()\n",
        "\n",
        "            # optimizer update\n",
        "            optimizer.step()\n",
        "\n",
        "            cost += loss # 총 600번의 loss를 더한다.\n",
        "\n",
        "        with torch.no_grad(): # 미분하지 않겠다\n",
        "            total = 0\n",
        "            correct = 0\n",
        "            for images, labels in dataset2_loader:\n",
        "                images = images.reshape(100,784)\n",
        "\n",
        "                outputs = model_dict[optimizer_name].forward(images)\n",
        "\n",
        "                # torch.max에서 두 번째 인자는 dim을 의미\n",
        "                # 1로 지정했다는 것은 하나의 행에서 가장 큰 값을 찾겠다는 것\n",
        "                # dim을 지정하면 인덱스에 있는 값과 인덱스를 반환\n",
        "                _,predict = torch.max(outputs, 1) \n",
        "\n",
        "                total += labels.size(0)\n",
        "                correct += (predict == labels).sum() # 예측한 것과 labels이 얼마나 일치하는지\n",
        "\n",
        "            avg_cost = cost / total_batch # loss 값 600개의 평균\n",
        "            accuracy = 100 * (correct/total) \n",
        "\n",
        "            loss_dict[optimizer_name].append(avg_cost.detach().numpy())\n",
        "            accuracy_dict[optimizer_name].append(accuracy)\n",
        "\n",
        "            print(\"epoch : {} | loss : {:.6f}\" .format(epoch, avg_cost))\n",
        "            print(\"Accuracy : {:.2f}\".format(100*correct/total))\n",
        "\n",
        "\n",
        "# +\n",
        "markers = {'SGD' : 'o', 'Adam' : 'x','AdaGrad' : 's', 'RMSprop' : 'D' }\n",
        "plt.figure(figsize = (10,5))\n",
        "plt.subplot(1,2,1)\n",
        "\n",
        "for key in optimizer_case:\n",
        "    plt.plot(epochs,loss_dict[key], marker = markers[key], markevery=100, label = key)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "for key in optimizer_case:\n",
        "    plt.plot(epochs, accuracy_dict[key],marker = markers[key], markevery=100, label=key)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "# -\n",
        "\n",
        "\n",
        "0 comments on commit 0d62a3f"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import os\n",
        "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
        "#https://hihack.tistory.com/entry/Pytorch-MNIST-데이터-셋-4가지-optimizer-성능-비교SGD-Adagrad-RMSprop-Adam [Goin0915:티스토리]"
      ],
      "metadata": {
        "id": "7mZ35DL0j1CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784,100)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(100,100)\n",
        "        self.fc3 = nn.Linear(100,10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x1 = self.fc1(x)\n",
        "        x2 = self.relu(x1)\n",
        "        x3 = self.fc2(x2)\n",
        "        x4 = self.relu(x3)\n",
        "        x5 = self.fc3(x4)\n",
        "        \n",
        "        return x5\n",
        "#https://hihack.tistory.com/entry/Pytorch-MNIST-데이터-셋-4가지-optimizer-성능-비교SGD-Adagrad-RMSprop-Adam [Goin0915:티스토리]"
      ],
      "metadata": {
        "id": "kYSbavjJkEhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_root = './MNIST_data'\n",
        "\n",
        "dataset1 = datasets.MNIST(root = download_root,\n",
        "                         train=True,\n",
        "                         transform = transforms.ToTensor(),\n",
        "                         download=True)\n",
        "\n",
        "dataset2 = datasets.MNIST(root=download_root,\n",
        "                         train=False,\n",
        "                         transform = transforms.ToTensor(),\n",
        "                         download=True)\n",
        "#https://hihack.tistory.com/entry/Pytorch-MNIST-데이터-셋-4가지-optimizer-성능-비교SGD-Adagrad-RMSprop-Adam [Goin0915:티스토리]"
      ],
      "metadata": {
        "id": "J8FqhD0XkKyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CsMYvIgYkTAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_s = 100\n",
        "# dataset1_loader의 len은 600\n",
        "# dataset2_loader의 len은 100\n",
        "dataset1_loader = DataLoader(dataset1, batch_size = batch_s)\n",
        "dataset2_loader = DataLoader(dataset2, batch_size = batch_s)\n",
        "#https://hihack.tistory.com/entry/Pytorch-MNIST-데이터-셋-4가지-optimizer-성능-비교SGD-Adagrad-RMSprop-Adam [Goin0915:티스토리]"
      ],
      "metadata": {
        "id": "8EoiF8gDkV2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dict = {}\n",
        "loss_dict = {}\n",
        "accuracy_dict = {}\n",
        "# optimizer에 따른 학습 정도를 살펴볼 4가지 테스트 케이스\n",
        "optimizer_case = ['SGD','Adam','AdaGrad','RMSprop']\n",
        "for key in optimizer_case:\n",
        "    model_dict[key] = Net()\n",
        "    loss_dict[key] = []\n",
        "    accuracy_dict[key] = []\n",
        "#출처: https://hihack.tistory.com/entry/Pytorch-MNIST-데이터-셋-4가지-optimizer-성능-비교SGD-Adagrad-RMSprop-Adam [Goin0915:티스토리]"
      ],
      "metadata": {
        "id": "tm4slUaxkZDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_dict = {}\n",
        "optimizer_dict['SGD'] = optim.SGD(model_dict['SGD'].parameters(),lr = 0.001 )\n",
        "optimizer_dict['Adam'] = optim.Adam(model_dict['Adam'].parameters(),lr= 0.001)\n",
        "optimizer_dict['AdaGrad'] = optim.Adagrad(model_dict['AdaGrad'].parameters(), lr=0.001)\n",
        "optimizer_dict['RMSprop'] = optim.RMSprop(model_dict['RMSprop'].parameters(),lr=0.001)\n",
        "#optimizer_dict['Adadelta'] = optim.Adadelta(model_dict['Adadelta'].parameters(),lr=0.001)\n",
        "#https://hihack.tistory.com/entry/Pytorch-MNIST-데이터-셋-4가지-optimizer-성능-비교SGD-Adagrad-RMSprop-Adam [Goin0915:티스토리]"
      ],
      "metadata": {
        "id": "dGAOZVg9kjEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_function, total batch size, epoch 정의\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "total_batch = len(dataset1_loader) # 600 (60000 / 100) => (train dataset / batch_size)\n",
        "epochs = np.arange(1,16)\n",
        "#출처: https://hihack.tistory.com/entry/Pytorch-MNIST-데이터-셋-4가지-optimizer-성능-비교SGD-Adagrad-RMSprop-Adam [Goin0915:티스토리]"
      ],
      "metadata": {
        "id": "C-yeL1QzmZC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 총 4가지 optimizer를 사용하여 학습\n",
        "for optimizer_name, optimizer in optimizer_dict.items():\n",
        "    print(optimizer_name)\n",
        "    for epoch in epochs:\n",
        "        cost=0\n",
        "        for images, labels in dataset1_loader: #dataloader는 image와 label로 구성\n",
        "            # 하나의 Tensor에 데이터 784(28x28)개가 담긴 리스트가 100개 들어있음\n",
        "            # 그리고 그것이 총 dataset1_loader의 len인 600개 존재\n",
        "            images = images.reshape(100,784) \n",
        "           \n",
        "            model_dict[optimizer_name].zero_grad()\n",
        "            \n",
        "            # feed forward\n",
        "            predict = model_dict[optimizer_name].forward(images)\n",
        "            \n",
        "            # loss 값 구하기\n",
        "            loss = loss_function(predict,labels) # 예측된 것과 label이 얼마나 차이가 나는지\n",
        "            \n",
        "            # back propagation\n",
        "            loss.backward()\n",
        "            \n",
        "            # optimizer update\n",
        "            optimizer.step()\n",
        "            \n",
        "            cost += loss # 총 600번의 loss를 더한다.\n",
        "            \n",
        "        with torch.no_grad(): # 미분하지 않겠다\n",
        "            total = 0\n",
        "            correct = 0\n",
        "            for images, labels in dataset2_loader:\n",
        "                images = images.reshape(100,784)\n",
        "                \n",
        "                outputs = model_dict[optimizer_name].forward(images)\n",
        "                \n",
        "                # torch.max에서 두 번째 인자는 dim을 의미\n",
        "                # 1로 지정했다는 것은 하나의 행에서 가장 큰 값을 찾겠다는 것\n",
        "                # dim을 지정하면 인덱스에 있는 값과 인덱스를 반환\n",
        "                _,predict = torch.max(outputs, 1) \n",
        "                \n",
        "                total += labels.size(0)\n",
        "                correct += (predict == labels).sum() # 예측한 것과 labels이 얼마나 일치하는지\n",
        "                \n",
        "            avg_cost = cost / total_batch # loss 값 600개의 평균\n",
        "            accuracy = 100 * (correct/total) \n",
        "            \n",
        "            loss_dict[optimizer_name].append(avg_cost.detach().numpy())\n",
        "            accuracy_dict[optimizer_name].append(accuracy)\n",
        "            \n",
        "            print(\"epoch : {} | loss : {:.6f}\" .format(epoch, avg_cost))\n",
        "            print(\"Accuracy : {:.2f}\".format(100*correct/total))\n",
        "#https://hihack.tistory.com/entry/Pytorch-MNIST-데이터-셋-4가지-optimizer-성능-비교SGD-Adagrad-RMSprop-Adam [Goin0915:티스토리]"
      ],
      "metadata": {
        "id": "0uRFmmPSmiLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "markers = {'SGD' : 'o', 'Adam' : 'x','AdaGrad' : 's', 'RMSprop' : 'D' }\n",
        "plt.figure(figsize = (10,5))\n",
        "plt.subplot(1,2,1)\n",
        "\n",
        "for key in optimizer_case:\n",
        "    plt.plot(epochs,loss_dict[key], marker = markers[key], markevery=100, label = key)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "for key in optimizer_case:\n",
        "    plt.plot(epochs, accuracy_dict[key],marker = markers[key], markevery=100, label=key)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "#출처: https://hihack.tistory.com/entry/Pytorch-MNIST-데이터-셋-4가지-optimizer-성능-비교SGD-Adagrad-RMSprop-Adam [Goin0915:티스토리]"
      ],
      "metadata": {
        "id": "Q7uziDOypnw-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}